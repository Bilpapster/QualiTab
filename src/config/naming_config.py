from enum import Enum

DELIMITER = '_'


class SamplesType(Enum):
    """
    Enum for different types of samples in the dataset.
    """
    CORRUPTED = 'COR'
    CLEAN = 'CLE'
    ALL = 'ALL'


class EvaluationTask(Enum):
    """
    Enum for different evaluation tasks.
    """
    LINEAR_PROBING = 'LP'
    K_MEANS_CLUSTERING = 'KMC'
    DISTANCE_FROM_REFERENCE = 'DFR'


class GroundTruthAssignment(Enum):
    """
    Enum for different types of ground truth labels.
    """
    TASK_LABELS = 'TLGT'
    CLEAN_CORRUPTED = 'CCGT'


class EvaluationMetric(Enum):
    """
    Enum for different evaluation metrics.
    """
    ACCURACY = 'ACC'
    PRECISION = 'PRC'
    RECALL = 'REC'
    F1 = 'F1'
    ROC_AUC = 'ROC_AUC'
    PURITY = 'PUR'
    SILHOUETTE = 'SIL'
    COSINE_SIMILARITY = 'COS'
    EUCLIDEAN_DISTANCE = 'ED'
    Z_NORM_EUCLIDEAN_DISTANCE = 'ZED'


class Scenario(Enum):
    """
    Enum for different scenarios in the experiment.
    """
    PERFECT_DATA = 'PD'
    PERFECT_CONTEXT = 'PC'
    ZERO_INTERVENTION = 'ZI'


def get_standardized_name_from_configurations(
    evaluation_task: EvaluationTask,
    samples_type: SamplesType | None = None,
    ground_truth_assignment: GroundTruthAssignment | None = None,
    evaluation_metric: EvaluationMetric | None = None,
    scenario: Scenario | None = None,
):
    """
    Generates a standardized name based on the provided configurations.
    The name is generated by concatenating the string representations of the configurations with a delimiter.
    """
    standardized_name = DELIMITER.join(filter(None, [
        evaluation_task.value,
        samples_type.value if samples_type else None,
        ground_truth_assignment.value if ground_truth_assignment else None,
        evaluation_metric.value if evaluation_metric else None,
        scenario.value if scenario else None
    ]))

    return standardized_name
